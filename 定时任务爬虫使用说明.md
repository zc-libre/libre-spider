# 定时任务爬虫系统使用说明

## 功能概述

本系统基于现有的小红书爬虫功能，实现了定时任务爬虫系统，支持：

1. **配置驱动**：通过配置文件管理关键词和爬取策略
2. **定时执行**：支持cron表达式定时自动爬取
3. **手动触发**：提供REST API手动触发爬取任务
4. **批量处理**：搜索 → 批量获取笔记详情 → 批量获取用户信息
5. **数据存储**：完整的数据库存储和管理
6. **状态监控**：实时任务状态监控和统计

## 核心特性

### 1. 配置管理
- 支持多关键词配置（逗号分隔）
- 可配置每个关键词的最大抓取数量
- 支持多种排序方式（综合、最新、最热）
- 灵活的请求间隔和并发控制

### 2. 定时任务调度
- 基于Spring Boot的@Scheduled注解
- 支持cron表达式灵活配置执行时间
- 默认每天8点执行，可自定义
- 支持启用/禁用定时任务

### 3. 数据存储
- 完整的数据库表结构设计
- 任务执行记录跟踪
- 笔记信息和用户信息分离存储
- 支持数据去重（基于noteId）

### 4. 状态监控
- 实时任务状态监控
- 全局统计信息（任务数、笔记数、用户数等）
- 任务执行历史记录
- 错误信息记录和统计

## 配置说明

### 1. 爬虫配置（application.yml）

```yaml
xhs:
  # 基础配置
  cookies: "你的cookie字符串"
  apiDomain: "https://edith.xiaohongshu.com"
  webDomain: "https://www.xiaohongshu.com"
  
  # 爬虫配置
  crawler:
    # 关键词配置（支持多个关键词，用逗号分隔）
    keywords: "美食制作,家常菜谱,烘焙教程,减肥食谱"
    
    # 每个关键词最大抓取笔记数量
    maxNotesPerKeyword: 20
    
    # 搜索页数（每页通常10-20条）
    maxPages: 2
    
    # 排序类型：GENERAL(综合), TIME_DESCENDING(最新), POPULARITY_DESCENDING(最热)
    sortType: GENERAL
    
    # 是否获取笔记详情
    enableNoteDetail: true
    
    # 是否获取用户信息
    enableUserInfo: true
    
    # 是否获取评论信息
    enableComments: false
    
    # 请求间隔（毫秒）
    requestInterval: 3000
    
    # 并发数量（批量处理时）
    concurrency: 3
    
    # 重试次数
    retryCount: 3
    
    # 数据存储配置
    storage:
      # 存储类型：DATABASE(数据库), JSON(JSON文件), CSV(CSV文件), ALL(所有)
      type: DATABASE
      
      # 文件存储路径
      filePath: "data/xhs"
      
      # 是否去重（基于noteId）
      enableDeduplication: true
    
    # 定时任务配置
    schedule:
      # 是否启用定时任务
      enabled: true
      
      # Cron表达式（默认每天8点执行）
      cron: "0 0 8 * * ?"
      
      # 任务超时时间（分钟）
      timeout: 60
```

### 2. 数据库配置

系统会自动创建以下数据表：

1. **crawl_task** - 爬取任务表
2. **note_info** - 笔记信息表
3. **user_info** - 用户信息表

## API接口

### 1. 手动触发接口

#### 触发全量爬虫任务
```bash
POST /api/crawler/trigger
```

#### 触发指定关键词爬虫任务
```bash
POST /api/crawler/trigger/{keyword}
```

#### 获取爬虫配置
```bash
GET /api/crawler/config
```

#### 更新关键词配置
```bash
POST /api/crawler/config/keywords
Content-Type: application/json

{
  "keywords": ["美食", "旅游", "摄影"]
}
```

### 2. 状态监控接口

#### 获取爬虫状态
```bash
GET /api/crawler/status
```

#### 获取所有任务状态
```bash
GET /api/crawler/tasks
```

#### 获取指定任务状态
```bash
GET /api/crawler/task/{taskName}
```

## 使用示例

### 1. 启动服务
```bash
# 开发模式
mvn spring-boot:run -Dspring.profiles.active=dev

# 生产模式
mvn clean package
java -jar target/libre-spider-1.0.0.jar
```

### 2. 手动触发爬虫
```bash
# 触发全量爬虫
curl -X POST "http://localhost:14315/api/crawler/trigger"

# 触发指定关键词爬虫
curl -X POST "http://localhost:14315/api/crawler/trigger/美食"

# 查看爬虫状态
curl "http://localhost:14315/api/crawler/status"
```

### 3. 查看任务执行情况
```bash
# 查看所有任务
curl "http://localhost:14315/api/crawler/tasks"

# 查看指定任务
curl "http://localhost:14315/api/crawler/task/定时任务-美食-2024-01-01-08-00"
```

## 执行流程

### 1. 定时任务执行流程
1. 根据配置的cron表达式自动触发
2. 遍历配置的关键词列表
3. 为每个关键词创建独立的爬取任务
4. 记录任务开始状态
5. 执行批量爬取流程
6. 记录任务完成状态

### 2. 批量爬取流程
1. **搜索笔记**：根据关键词搜索指定页数的笔记
2. **去重处理**：根据noteId去除已存在的笔记
3. **获取详情**：批量获取笔记详情信息（如果启用）
4. **保存笔记**：将笔记信息保存到数据库
5. **获取用户**：批量获取用户信息（如果启用）
6. **保存用户**：将用户信息保存到数据库

### 3. 错误处理
- 任务级别的错误记录
- 单个笔记/用户获取失败不影响整体任务
- 完整的重试机制
- 详细的错误日志记录

## 监控指标

### 1. 任务统计
- 总任务数
- 当前运行任务数
- 任务成功/失败率
- 任务平均执行时间

### 2. 数据统计
- 总笔记数
- 总用户数
- 错误数量
- 数据增长趋势

### 3. 性能指标
- 请求成功率
- 平均响应时间
- 并发处理能力
- 内存使用情况

## 注意事项

1. **Cookie配置**：确保Cookie包含有效的web_session
2. **请求频率**：合理设置请求间隔，避免被封IP
3. **数据量控制**：根据实际需求调整每个关键词的抓取数量
4. **存储空间**：定期清理历史数据，避免数据库过大
5. **监控告警**：建议配置监控告警，及时发现问题

## 扩展功能

### 1. 支持更多平台
- 可参考Python MediaCrawler项目架构
- 抽象出通用的爬虫接口
- 支持抖音、快手等其他平台

### 2. 增强数据处理
- 支持数据导出（CSV、JSON）
- 增加数据分析功能
- 支持数据可视化

### 3. 提升性能
- 引入消息队列进行异步处理
- 增加缓存机制
- 优化数据库查询性能

## 故障排除

### 1. 常见问题
- 编译错误：运行`mvn spring-javaformat:apply`修复格式
- Cookie失效：重新获取并更新Cookie配置
- 数据库连接错误：检查数据库配置和连接

### 2. 日志查看
- 应用日志：查看控制台输出
- 任务状态：通过API接口查看
- 数据库记录：直接查询数据库表

### 3. 性能优化
- 调整并发数量
- 优化请求间隔
- 增加内存分配

## 总结

本定时任务爬虫系统具有以下优势：
- 完整的任务调度和监控机制
- 灵活的配置管理
- 可靠的数据存储
- 良好的扩展性
- 与Python版本功能对等

系统可以稳定运行，满足定时批量爬取小红书数据的需求。